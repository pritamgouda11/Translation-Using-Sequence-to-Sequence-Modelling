# -*- coding: utf-8 -*-
"""Gouda_Pritam_Trilochan_Savita_23754_assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qbvDCNaFOJE7SnlyVwI5OeVU3j8lrsR9

# Assignment 3: Sequence-to-Sequence Modeling (TA: Kinshuk Vasisht)

In this assignment, you will perform the task of translating Indian Names to Hindi, a sequence-to-sequence modeling task, using character-level conditional language models.

As before, please make a copy of this notebook (locally or on Colab). Ensure you adhere to the guidelines and submission instructions (mentioned below) for attempting and submitting the assignment.

### Guidelines for Attempting the Assignment

1. Write your logic in the cells which have the comment `# ADD YOUR CODE HERE`, between the `# BEGIN CODE` and `# END CODE` comments. These cells are also demarcated by the special start (`## ==== BEGIN EVALUATION PORTION`) and end (`## ==== END EVALUATION PORTION`) comments. Do **NOT** remove any of these comments from the designated cells, otherwise your assigment may not be evaluated correctly.

2. All imports that should be necessary are already provided as part of the notebook. Should you require additional imports, add them in the cells to be graded, but outside the `# BEGIN CODE` and `# END CODE` block. For example, if you need to import a package called `mypackage`, add it as follows in a graded cell:

``` python
## ==== BEGIN EVALUATION PORTION

import mypackage # <===

def function_to_be_implemented(*args, **kwargs):

    ...

    # ADD YOUR CODE HERE
    # BEGIN CODE

    # END CODE

    ...

## ==== END EVALUATION PORTION

```

3. Only write your code in the cells designated for auto-evaluation. If you encounter any errors in the supporting cells during execution, contact the respective TAs.

4. **Important**: Use of AI-assistive technologies such as ChatGPT or GitHub CoPilot is not permitted for this assignment. Ensure that all attempts are solely your own. Not following this rule can incur heavy penalty, including getting NO GRADE for this assignment, which will affect your grade significantly.

### Submission Instructions

1. Ensure your code follows all guidelines mentioned above before submission.

2. Try to avoid any unnecessary print statements across the code. We will evaluate specific output lines which begin with the phrase `EVALUATION`. Ensure you do not modify these print statements howsoever, as they are used for auto-evaluation.

3. When you have completely attempted the assignment, export the current notebook as a `.py` file, with the following name: `SAPName_SRNo_assignment3.py`, where `SAPName` would be your name as per SAP record, and `SRNo` will be the last 5 digits of your IISc SR number. For example, IISc student with SAP name Twyla Linda (SR no - 04-03-00-10-22-20-1-15329) would use `Twyla_Linda_15329_assignment3.py`.

4. Once you have executed the code, certain additional files will be created. Once you are done executing all associated cells, ensure the folder structure looks as follows:

``` python
â””â”€â”€â”€ SAPName_SRNo
     â”œâ”€â”€â”€ SAPName_SRNo_assignment3.py
     â”œâ”€â”€â”€ src-tokenizer
     â”‚    â””â”€â”€â”€ tokenizer.pkl
     â”œâ”€â”€â”€ tgt-tokenizer
     â”‚    â””â”€â”€â”€ tokenizer.pkl
     â”œâ”€â”€â”€ rnn.enc-dec
     â”‚    â”œâ”€â”€â”€ model.pt
     â”‚    â”œâ”€â”€â”€ loss.json
     â”‚    â”œâ”€â”€â”€ outputs.csv
     â”‚    â””â”€â”€â”€ metadata.json
     â””â”€â”€â”€ rnn.enc-dec.attn
          â”œâ”€â”€â”€ model.pt
          â”œâ”€â”€â”€ loss.json
          â”œâ”€â”€â”€ outputs.csv
          â””â”€â”€â”€ metadata.json
```

5. Once you have validated the folder structure as above, add the exported `.py` file to the folder and submit the folder as a ZIP archive.

In the cell below, replace `SAPName` with your name as per SAP record, and `SRNo` with the last 5 digits of your IISc SR number. For example, IISc student with SAP name Twyla Linda (SR no - 04-03-00-10-22-20-1-15329) would use:
"""

STUDENT_SAP_NAME  = "GOUDA_PRITAM_TRILOCHAN_SAVITA"
STUDENT_SR_NUMBER = "23754"

"""I have also implemented LSTM, BPE

**Reason:** while implementing attention part i started getting "Found no driver on your system. Please check that you have an GPU and installed" but it was working just fine and suddenly it wasn't, so when I tried on my friends machine it worked ðŸ˜•.

***So I have added link to my LSTM with BPE code aswell after "Congratulations" section***

I have added my previous code as I felt it is helpful for me to compare the outputs

**Important Notes**:

- Some of the tasks in this assignment are compute intensive, and are better performed on an accelerator device (GPU, etc.). Unless you have one locally, prefer using a GPU instance on Colab for execution.
- Due to resource restrictions on Colab, training some models may not finish in time. In such a case, ensure you store checkpoints to a persistent directory so that you may resume training once your resource limits are restored.

## Outline

Through the last assignment, you have seen that neural language models are able to successfully capture patterns across Indian names. In this assignment, you will extend upon that idea to learn conditional language models for the task of transliteration: converting Indian names in the English alphabet to Hindi.

### Marks Distribution

- Tokenization: 20 marks
- Agnostic Task-Specific Training: 5 marks
- Seq-2-Seq via RNN: 40 marks
- Seq-2-Seq via RNN with Attention: 35 marks
- Evaluation
- (**Bonus**) Decoding Strategies: 20 marks

## Setup

The following cells perform the basic setup such as importing the necessary packages.
"""

# Commented out IPython magic to ensure Python compatibility.
# Installs packages, if using locally. Feel free to add other missing packages as required.

# %pip install tqdm nltk matplotlib numpy pandas

# Built-in imports, no installations required.

import os
import gc
import json
import math
import pickle
import subprocess
import collections
import unicodedata

# 3rd-party package imports, may require installation if not on a platform such as Colab.

import numpy
import torch
import pandas as pd
import tqdm.auto as tqdm
import torch.nn as nn

import matplotlib
from matplotlib import pyplot
from nltk.translate import bleu_score
from torch.utils.data import TensorDataset, DataLoader

# Please do not change anything in the following cell

# Find and load fonts that can display Hindi characters, for Matplotlib
result = subprocess.run([ 'fc-list', ':lang=hi', 'family' ], capture_output=True)
found_hindi_fonts = result.stdout.decode('utf-8').strip().split('\n')

matplotlib.rcParams['font.sans-serif'] = [
    'Source Han Sans TW', 'sans-serif', 'Arial Unicode MS',
    *found_hindi_fonts
]

# Please do not change anything in the following cell

DIRECTORY_NAME = f"{STUDENT_SAP_NAME.replace(' ', '_')}_{STUDENT_SR_NUMBER}"

os.makedirs(DIRECTORY_NAME, exist_ok=True)

def sync_vram():
    """ Synchronizes the VRAM across the GPUs, reclaiming unused memory. """
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()

"""## Data Preparation

We'll load the data for the task, which comprises of a parallel corpus of Indian Names and their Hindi equivalents.
"""

# Make sure your code is not dependent on any of the file names as below.

# Download the training and validation datasets
!wget -O data.train.csv "https://docs.google.com/spreadsheets/d/1JpK9nOuZ2ctMrjNL-C0ghUQ4TesTrMER1-dTD_torAA/gviz/tq?tqx=out:csv&sheet=data.train.csv"
!wget -O data.valid.csv "https://docs.google.com/spreadsheets/d/1cKC0WpWpIQJkaqnFb7Ou7d0syFDsj6eEW7bM7GH3u2k/gviz/tq?tqx=out:csv&sheet=data.valid.csv"
def read_dataframe(ds_type):
    """ Loads a dataframe based on the given partition type.

    Args:
        ds_type (str): Dataset type: train (train) or validation (valid)

    Returns:
        pd.DataFrame: Pandas Dataframe for the specified partition.
    """

    df = pd.read_csv(f"data.{ds_type}.csv", header=0)
    df = df[~df.isna()]
    df['Name'] = df['Name'].astype(str)
    df['Translation'] = df['Translation'].astype(str)
    return df

# Load the training and validation datasets
train_data      = read_dataframe("train")
validation_data = read_dataframe("valid")

print(f"Length of training data: {len(train_data)}\nLength of validation data: {len(validation_data)}")

"""Here are some examples from the training dataset. Note that the dataset may be noisy so some examples may not be perfect:"""

train_data.sample(n=5)

#more info on our data
print("about training data:")
print(train_data.info())

print("\n about validation data:")
print(validation_data.info())

def count_pairs(items):
    counts = {}
    for pair in zip(items, items[1:]):
        counts[pair] = counts.get(pair, 0) + 1
    return counts

def combine(items, pair_to_combine, newt):
    combined = []
    i = 0
    while i < len(items):
        if i < len(items) - 1 and items[i] == pair_to_combine[0] and items[i+1] == pair_to_combine[1]:
            combined.append(newt)
            i += 2
        else:
            combined.append(items[i])
            i += 1
    return combined
mergerules = {}
namecol = train_data["Name"].to_list()
transcol = train_data["Translation"].to_list()
names=namecol
iter1 =[]
iter2=[]
for name in names:
    for x in name:
        iter1.append(ord(x))
for name in names:
    iter2.append(list(name.encode('utf-8')))
text = [item for sublist in names for item in sublist]
tokens = [item for sublist in iter2 for item in sublist]
vocab_size = 290
merge_count = vocab_size - 256
ids = list(tokens)
for i in range(merge_count):
    pair_stats = count_pairs(ids)
    top_pair = max(pair_stats, key=pair_stats.get)
    new_id = 256 + i
    ids = combine(ids, top_pair, new_id)
    mergerules[top_pair] = new_id
def encode(text):
  tokens = list(text.encode("utf-8"))
  while len(tokens) >= 2:
    stats = count_pairs(tokens)
    pair = min(stats, key=lambda p: mergerules.get(p, float("inf")))
    if pair not in mergerules:
      break
    idx = mergerules[pair]
    tokens = combine(tokens, pair, idx)
  return tokens

def make_text(ids):
    bits = b"".join(vocab[id] for id in ids)
    text = bits.decode("utf-8", errors="ignore")
    return text

def pull_apart(word):
    return [char for char in word]



vocab = {index: bytes([index]) for index in range(256)}
for (first, second), idx in mergerules.items():
    vocab[idx] = vocab[first] + vocab[second]
print(vocab[256])
print(str(vocab[256].decode('utf-8',errors="replace")))

for name in ["pritam", "gouda"]:
    encoded = encode(name)
    decoded = make_text(encoded)
    assert(decoded == name)
for name in namecol:
    encoded = encode(name)
    decoded = make_text(encoded)
    assert(decoded == name)

special_token = '\x04'
encoded_special = encode(special_token)
subset_example = tokens[:3]
# END CODE
class Tokenizer:
    """ Represents the tokenizer for text data.
        Provides methods to encode and decode strings (as instance or as a batch). """

    def __init__(self,data):
        """ Initializes a new tokenizer.

            Any variables required in intermediate operations are declared here.
            You will also need to define things like special tokens and other things here.

            All variables declared in this function will be serialized
                and deserialized when loading and saving the Tokenizer.
            """

        # BEGIN CODE : tokenizer.init

        # ADD YOUR CODE HERE
        self.vocab ={}
        self.merges = {}
        self.get_vocab={}
        self.SOT_token = b'\x01'
        self.EOT_token = b'\x04'
        self.pad_token = b'\x00'
        self.add_start = False
        self.add_end =False
        self.strip_special = False

        # END CODE

    @classmethod
    def load(cls, path):
        """ Loads a pre-trained tokenizer from the given directory.
           This directory will have a tokenizer.pkl file that contains all the tokenizer variables.

        Args:
            path (str): Path to load the tokenizer from.
        """
        tokenizer_file = os.path.join(path, "tokenizer.pkl")

        if not os.path.exists(path) or not os.path.exists(os.path.join(path, "tokenizer.pkl")):
            raise ValueError(cls.load.__name__ + ": No tokenizer found at the specified directory")

        with open(tokenizer_file, "rb") as ifile:
            return pickle.load(ifile)

    def save(self, path):
        """ Saves a trained tokenizer to a given directory, inside a tokenizer.pkl file.

        Args:
            path (str): Directory to save the tokenizer in.
        """

        os.makedirs(path, exist_ok=True)
        with open(os.path.join(path, "tokenizer.pkl"), 'wb') as ofile:
            pickle.dump(self, ofile)

    def get_stats(self,ids):
        counts = {}
        for pair in zip(ids, ids[1:]):
            counts[pair] = counts.get(pair, 0) + 1
        return counts

    def merge(self,ids, pair, idx):
        newids = []
        i = 0
        while i < len(ids):
            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:
                newids.append(idx)
                i += 2
            else:
                newids.append(ids[i])
                i += 1
        return newids

    def train(self, data, vocab_size):
        """ Trains a tokenizer to learn meaningful representations from input data.
            In the end, learns a vocabulary of a fixed size over the given data.
            Special tokens, if any, must not be counted towards this vocabulary.

        Args:
            data (list[str]): List of input strings from a text corpus.
            vocab_size (int): Final desired size of the vocab to be learnt.
        """

        # BEGIN CODE : tokenizer.train
        # ADD YOUR CODE HERE
        names = data
        i=[]
        for name in names:
            i.append(list(name.encode('utf-8')))
        text = [item for sublist in names for item in sublist]
        tokens = [item for sublist in i for item in sublist]

        vocab_size = vocab_size
        num_merges = vocab_size - 256
        ids = list(tokens)
        for i in range(num_merges):
            stats = self.get_stats(ids)
            pair = max(stats, key=stats.get)
            idx = 256 + i
            ids = self.merge(ids, pair, idx)
            self.merges[pair] = idx
        self.vocab = {idx: bytes([idx]) for idx in range(256)}
        for (p0, p1), idx in self.merges.items():
            self.vocab[idx] = self.vocab[p0] + self.vocab[p1]
        # END CODE


    def pad(self, tokens, length):
        """ Pads a tokenized string to a specified length, for batch processing.

        Args:
            tokens (list[int]): Encoded token string to be padded.
            length (int): Length of tokens to pad to.

        Returns:
            list[int]: Token string padded to desired length.
        """

        # BEGIN CODE : tokenizer.pad

        # ADD YOUR CODE HERE
        if len(tokens) >= length:
            return tokens
        pad_length = length - len(tokens)

        start_padding =[]
        end_padding =[]
        padding = []
        get_vocab = self.get_vocabulary()
        if self.add_start and self.add_end:
            padding = [get_vocab[self.pad_token]] *pad_length
        else:
            padding = [get_vocab[self.pad_token]] * pad_length

        padded = tokens[:-1]+padding
        end_padding.append(tokens[-1])
        padded_tokens = start_padding + padded + end_padding
        return padded_tokens

        # END CODE

    def unpad(self, tokens):
        """ Removes padding from a token string.

        Args:
            tokens (list[int]): Encoded token string with padding.

        Returns:
            list[int]: Token string with padding removed.
        """

        # BEGIN CODE : tokenizer.unpad

        # ADD YOUR CODE HERE
        try:
            if self.strip_special:
                tokens = tokens[1:]
                tokens = tokens[:-1]
            first_pad_index = tokens.index(self.pad_token)
            tokens = tokens[:first_pad_index]
        except ValueError:
            return tokens

        # END CODE

    def get_special_tokens(self):
        """ Returns the associated special tokens.

            Returns:
                dict[str, int]: Mapping describing the special tokens, if any.
                    This is a mapping between a string segment (token) and its associated id (token_id).
        """

        # BEGIN CODE : tokenizer.get_special_tokens

        # ADD YOUR CODE HERE
        get_vocab =self.get_vocabulary()
        token_dict = {self.SOT_token : get_vocab[self.SOT_token],self.EOT_token : get_vocab[self.EOT_token]}
        return token_dict
        # END CODE
